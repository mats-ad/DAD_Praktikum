version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "29092:29092" # Host -> Kafka
      - "9092:9092" # optional
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"

      # intern: 9092, extern: 29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: ["CMD-SHELL", "cub kafka-ready -b localhost:9092 1 20 || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 20

  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: gabi
      POSTGRES_USER: gabi
      POSTGRES_PASSWORD: gabi_pw
    ports:
      - "5432:5432"
    volumes:
      - gabi_pgdata:/var/lib/postgresql/data
      - ./serving/schema.sql:/docker-entrypoint-initdb.d/schema.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U gabi -d gabi"]
      interval: 5s
      timeout: 5s
      retries: 20

  api:
    build:
      context: .
      dockerfile: serving/Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "8000:8000"
    environment:
      PG_HOST: postgres
      PG_PORT: "5432"
      PG_DB: gabi
      PG_USER: gabi
      PG_PASSWORD: gabi_pw

  joined_to_postgres:
    build:
      context: .
      dockerfile: jobs/joined_to_postgres/Dockerfile
    container_name: gabi_joined_to_postgres
    environment:
      # Parquet im gemounteten /data
      JOINED_PATH: /data/lake/joined/daily

      # JDBC im Container: postgres (Service-Name), nicht localhost
      PG_URL: jdbc:postgresql://postgres:5432/gabi
      PG_USER: gabi
      PG_PASSWORD: gabi_pw
      PG_TABLE: joined_daily

      # alle 5 Minuten neu schreiben
      INTERVAL_SECONDS: "300"
    volumes:
      - ./data:/data
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    container_name: gabi_prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    depends_on:
      kafka_exporter:
        condition: service_started
      postgres_exporter:
        condition: service_started
      pipeline_metrics:
        condition: service_started

  grafana:
    image: grafana/grafana:latest
    container_name: gabi_grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - postgres

  kafka_exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: gabi_kafka_exporter
    command:
      - "--kafka.server=kafka:9092"
    ports:
      - "9308:9308"
    depends_on:
      kafka:
        condition: service_healthy

  postgres_exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: gabi_postgres_exporter
    environment:
      DATA_SOURCE_NAME: postgresql://gabi:gabi_pw@postgres:5432/gabi?sslmode=disable
    ports:
      - "9187:9187"
    depends_on:
      postgres:
        condition: service_healthy

  pipeline_metrics:
    build:
      context: ./monitoring/pipeline_metrics
    container_name: gabi_pipeline_metrics
    environment:
      KAFKA_BROKER: kafka:9092
      POSTGRES_DSN: postgresql://gabi:gabi_pw@postgres:5432/gabi
      LAKE_ROOT: /data/lake
    volumes:
      - ./data:/data
    ports:
      - "8001:8000"
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy

  scheduler:
    build:
      context: .
      dockerfile: jobs/scheduler/Dockerfile
    container_name: gabi_scheduler
    environment:
      LAKE_ROOT: /data/lake

      # Spark Jobs brauchen JDBC url/creds:
      PG_URL: jdbc:postgresql://postgres:5432/gabi
      PG_USER: gabi
      PG_PASSWORD: gabi_pw
      PG_TABLE: joined_daily

      # psql (investment_signal.sql)
      POSTGRES_HOST: postgres
      POSTGRES_DB: gabi
      POSTGRES_USER: gabi
      POSTGRES_PASSWORD: gabi_pw
    volumes:
      - ./data:/data
      - ./batch:/app/batch:ro
      - ./serving:/app/serving:ro
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  gabi_pgdata:
